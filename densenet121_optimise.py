# -*- coding: utf-8 -*-
"""densenet121_optimise.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QvknHX4tvWBy_MZBIcESN3fS7RQY0V8i
"""

!pip install split-folders

! pip install kaggle
from google.colab import files
files.upload()
! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/

import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Flatten, Conv2D, BatchNormalization, MaxPool2D, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.preprocessing import image
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import cv2
import splitfolders
from glob import glob

!kaggle datasets download -d raddar/tuberculosis-chest-xrays-shenzhen

! unzip /content/tuberculosis-chest-xrays-shenzhen.zip

datainfo = pd.read_csv('/content/shenzhen_metadata.csv')
datainfo.head()

normal = []
positive =  []

def extract_target(x):
  for i in range(len(x['study_id'])):
    if x['findings'][i] == 'normal':
      normal.append(x['study_id'][i])
    else:
      positive.append(x['study_id'][i])

extract_target(datainfo)

len(normal)

len(positive)

!mkdir data
!mkdir data/normal
!mkdir data/positive

for i in range(len(normal)):
 path = '/content/images/images/' + normal[i]
 !mv $path /content/data/normal

for i in range(len(positive)):
  path = '/content/images/images/' + positive[i]
  !mv $path /content/data/positive

input_folder = "/content/data"
output = "/content/dataset" #where you want the split datasets saved. one will be created if none is set

splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .0, 0.2))

tuberculosis  = glob('/content/dataset/test/positive/*.png')
normal = glob('/content/dataset/test/normal/*.png')

!pip install torchvision

import torch
import torchvision
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from sklearn.model_selection import train_test_split

# Hyperparameters (adjust as needed)
learning_rate = 0.0001
batch_size = 32
num_epochs = 50

# Data paths (modify these)
data_dir = '/content/dataset/'
train_dir = '/content/dataset/train'
val_dir = '/content/dataset/test'  # Assuming you have a validation set

# Define transformation with resizing and center crop
transform = transforms.Compose([
    transforms.Resize(256),  # Resize the shorter side of the image to 256 pixels
    transforms.CenterCrop(224),  # Crop the center 224x224 region
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Load datasets
train_dataset = datasets.ImageFolder(train_dir, transform=transform)
val_dataset = datasets.ImageFolder(val_dir, transform=transform)

# Data loaders
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

# Load pre-trained DenseNet-121
model = torchvision.models.densenet121(pretrained=True)

# Freeze pre-trained layers
for param in model.parameters():
    param.requires_grad = False

# Modify classifier head
num_features = model.classifier.in_features  # Get the number of input features to the classifier
model.classifier = nn.Sequential(
    nn.Linear(num_features, 64),  # Adjust output channels as needed
    nn.ReLU(inplace=True),
    nn.Linear(64, 1),  # Output single value for binary classification
    nn.Sigmoid()
)

# Define loss function and optimizer
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Training loop
for epoch in range(num_epochs):
    running_loss = 0.0
    for i, data in enumerate(train_loader, 0):
        inputs, labels = data

        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = model(inputs)

        # Calculate loss
        loss = criterion(outputs.squeeze(), labels.float())  # Squeeze output and convert labels to float

        # Backward pass and optimize
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        if i % 100 == 99:    # Print every 100 mini-batches
            print(f'Epoch [{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}')
            running_loss = 0.0

print('Finished Training')

correct = 0
total = 0
model.eval()  # Set the model to evaluation mode
with torch.no_grad():  # Disable gradient tracking during validation
    for data in val_loader:
        inputs, labels = data
        outputs = model(inputs)
        predicted = (outputs > 0.5).float()  # Convert probabilities to binary predictions
        total += labels.size(0)
        correct += (predicted.squeeze() == labels.float()).sum().item()

accuracy = 100 * correct / total
print(f'Accuracy on the validation set: {accuracy:.2f}%')

import torch
import torchvision.models as models

# Load pre-trained DenseNet-121
model = models.densenet121(pretrained=True)

# Print the DenseNet-121 architecture
print(model)

import torch

# Save the PyTorch model
torch.save(model.state_dict(), "TBdetectionfinal2.pt")

import torch

# Load the PyTorch model
model = torch.load("TBdetectionfinal2.pt")

torch.save(model, 'tb.pth')  # Save the entire model
print('Model saved successfully!')

